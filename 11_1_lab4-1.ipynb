{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-04 Multivariable Linear regression and Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전에는 Simple Linear regression이었다면 이번에는 여러개의 변수에 대해 하나의 예측값이 나오게 하는 Multivariable Linear regression을 해보려고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                            [93, 88, 93],\n",
    "                            [89, 91, 90],\n",
    "                            [96, 98, 100],\n",
    "                            [73, 66, 70]])\n",
    "\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Hypothesis\n",
    "$$y=Wx+b$$\n",
    "Simple Linear Regression과 식자체는 일치하지만 $x$가 scalar 값에서 vector로 바뀐 것이다.\n",
    "\n",
    "예를 들어 입력 변수가 3개라고 했을때, $$y=w_1x_1+w_2x_2+w_3x_3+b$$라고 볼 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100/1000, hypothesis: tensor([152.7695, 183.6982, 180.9592, 197.0628, 140.1332]), Cost:1.564299\n",
      "Epoch  200/1000, hypothesis: tensor([152.7277, 183.7271, 180.9466, 197.0518, 140.1727]), Cost:1.498234\n",
      "Epoch  300/1000, hypothesis: tensor([152.6870, 183.7551, 180.9344, 197.0410, 140.2112]), Cost:1.435647\n",
      "Epoch  400/1000, hypothesis: tensor([152.6474, 183.7825, 180.9225, 197.0305, 140.2487]), Cost:1.376296\n",
      "Epoch  500/1000, hypothesis: tensor([152.6089, 183.8091, 180.9109, 197.0202, 140.2852]), Cost:1.320047\n",
      "Epoch  600/1000, hypothesis: tensor([152.5714, 183.8349, 180.8997, 197.0102, 140.3208]), Cost:1.266736\n",
      "Epoch  700/1000, hypothesis: tensor([152.5350, 183.8601, 180.8888, 197.0004, 140.3554]), Cost:1.216203\n",
      "Epoch  800/1000, hypothesis: tensor([152.4995, 183.8846, 180.8781, 196.9908, 140.3891]), Cost:1.168279\n",
      "Epoch  900/1000, hypothesis: tensor([152.4651, 183.9085, 180.8678, 196.9815, 140.4220]), Cost:1.122853\n",
      "Epoch 1000/1000, hypothesis: tensor([152.4315, 183.9316, 180.8578, 196.9724, 140.4540]), Cost:1.079796\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((3,1), requires_grad = True)\n",
    "b = torch.zeros(1,requires_grad = True)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr = 1e-5)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(1,epochs+1):\n",
    "    \n",
    "    # matmul을 이용하여 hypothesis를 표현하였다. \n",
    "    hypothesis = x_train.matmul(W)+b\n",
    "    \n",
    "    # 이후 행동은 simple linear regression과 동일하다.\n",
    "    cost = torch.mean((y_train-hypothesis)**2)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch %100 ==0:\n",
    "        print('Epoch {:4d}/{}, hypothesis: {}, Cost:{:.6f}'.format(\n",
    "        epoch,epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.module을 사용하여 모델을 만들어 보자!\n",
    "\n",
    "nn.module의 경우 custom한 모델을 만들기 위해서 상속받는 것이며, forward을 정의해서 자신만의 모듈을 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Multi_Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ## nn이라는 library안에 Linear함수가 이미 내장되어 있어 그것을 꺼내 사용하고, (input size, output size)를 의미한다.\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        \n",
    "    # forward에서 hypothesis를 계산한다고 보면 된다.\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.functional library를 이용하여 loss함수를 불러보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# mse loss함수 또한 이미 내장되어있다. 이런식으로 할 경우 내장되어있는 다른 함수와 교체가 쉬워진다.\n",
    "cost = F.mse_loss(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그렇다면 바꾼 것들을 이용하여 모델을 새로 코딩해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100/1000, hypothesis: tensor([154.4220, 182.4948, 181.3660, 198.0656, 137.9721]), Cost:6.899693\n",
      "Epoch  200/1000, hypothesis: tensor([154.3283, 182.5594, 181.3378, 198.0415, 138.0600]), Cost:6.571748\n",
      "Epoch  300/1000, hypothesis: tensor([154.2371, 182.6223, 181.3103, 198.0181, 138.1455]), Cost:6.261016\n",
      "Epoch  400/1000, hypothesis: tensor([154.1485, 182.6835, 181.2836, 197.9951, 138.2288]), Cost:5.966525\n",
      "Epoch  500/1000, hypothesis: tensor([154.0622, 182.7429, 181.2577, 197.9727, 138.3099]), Cost:5.687458\n",
      "Epoch  600/1000, hypothesis: tensor([153.9783, 182.8008, 181.2324, 197.9509, 138.3889]), Cost:5.422935\n",
      "Epoch  700/1000, hypothesis: tensor([153.8967, 182.8571, 181.2079, 197.9296, 138.4659]), Cost:5.172279\n",
      "Epoch  800/1000, hypothesis: tensor([153.8172, 182.9120, 181.1840, 197.9087, 138.5408]), Cost:4.934695\n",
      "Epoch  900/1000, hypothesis: tensor([153.7400, 182.9653, 181.1608, 197.8884, 138.6138]), Cost:4.709536\n",
      "Epoch 1000/1000, hypothesis: tensor([153.6649, 183.0172, 181.1382, 197.8685, 138.6848]), Cost:4.496109\n"
     ]
    }
   ],
   "source": [
    "######## W = torch.zeros((3,1), requires_grad = True)\n",
    "######## b = torch.zeros(1,requires_grad = True)\n",
    "model = Multi_Linear()\n",
    "\n",
    "\n",
    "######## optimizer = optim.SGD([W, b], lr = 1e-5)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(1,epochs+1):\n",
    "    \n",
    "    ######## hypothesis = x_train.matmul(W)+b\n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    ######## cost = torch.mean((y_train-hypothesis)**2)\n",
    "    cost = F.mse_loss(hypothesis, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch %100 ==0:\n",
    "        print('Epoch {:4d}/{}, hypothesis: {}, Cost:{:.6f}'.format(\n",
    "        epoch,epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([14.7394, 18.4595, 17.8038, 19.7524, 13.9007]) Cost : 24071.289062\n",
      "Epoch    1/20 hypothesis: tensor([75.3282, 91.2835, 89.5581, 97.8911, 69.4472]) Cost : 7546.069824\n",
      "Epoch    2/20 hypothesis: tensor([109.2495, 132.0550, 129.7307, 141.6380, 100.5458]) Cost : 2366.290283\n",
      "Epoch    3/20 hypothesis: tensor([128.2406, 154.8817, 152.2218, 166.1303, 117.9569]) Cost : 742.704346\n",
      "Epoch    4/20 hypothesis: tensor([138.8729, 167.6615, 164.8137, 179.8425, 127.7049]) Cost : 233.795853\n",
      "Epoch    5/20 hypothesis: tensor([144.8253, 174.8166, 171.8634, 187.5195, 133.1626]) Cost : 74.279953\n",
      "Epoch    6/20 hypothesis: tensor([148.1578, 178.8226, 175.8102, 191.8175, 136.2184]) Cost : 24.279486\n",
      "Epoch    7/20 hypothesis: tensor([150.0233, 181.0656, 178.0198, 194.2237, 137.9293]) Cost : 8.606831\n",
      "Epoch    8/20 hypothesis: tensor([151.0676, 182.3214, 179.2569, 195.5708, 138.8874]) Cost : 3.693863\n",
      "Epoch    9/20 hypothesis: tensor([151.6521, 183.0246, 179.9494, 196.3250, 139.4239]) Cost : 2.153531\n",
      "Epoch   10/20 hypothesis: tensor([151.9791, 183.4184, 180.3371, 196.7472, 139.7244]) Cost : 1.670368\n",
      "Epoch   11/20 hypothesis: tensor([152.1621, 183.6390, 180.5541, 196.9835, 139.8929]) Cost : 1.518550\n",
      "Epoch   12/20 hypothesis: tensor([152.2643, 183.7626, 180.6755, 197.1157, 139.9873]) Cost : 1.470603\n",
      "Epoch   13/20 hypothesis: tensor([152.3214, 183.8319, 180.7434, 197.1897, 140.0403]) Cost : 1.455219\n",
      "Epoch   14/20 hypothesis: tensor([152.3533, 183.8709, 180.7814, 197.2311, 140.0702]) Cost : 1.450030\n",
      "Epoch   15/20 hypothesis: tensor([152.3709, 183.8928, 180.8027, 197.2543, 140.0871]) Cost : 1.448048\n",
      "Epoch   16/20 hypothesis: tensor([152.3806, 183.9051, 180.8145, 197.2671, 140.0967]) Cost : 1.447067\n",
      "Epoch   17/20 hypothesis: tensor([152.3859, 183.9122, 180.8211, 197.2743, 140.1022]) Cost : 1.446396\n",
      "Epoch   18/20 hypothesis: tensor([152.3887, 183.9162, 180.8247, 197.2783, 140.1055]) Cost : 1.445824\n",
      "Epoch   19/20 hypothesis: tensor([152.3901, 183.9186, 180.8267, 197.2805, 140.1074]) Cost : 1.445300\n",
      "Epoch   20/20 hypothesis: tensor([152.3907, 183.9201, 180.8278, 197.2816, 140.1087]) Cost : 1.444751\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    hypo = model(x_train)\n",
    "#     hypo = x_train.matmul(W) + b\n",
    "    cost = F.mse_loss(hypo, y_train)\n",
    "#     cost = torch.mean((hypo - y_train) ** 2)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"Epoch {:4d}/{} hypothesis: {} Cost : {:.6f}\".\n",
    "          format(epoch, nb_epochs, hypo.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = [[73, 80, 75],\n",
    "                      [93, 88, 93],\n",
    "                      [89, 91, 90],\n",
    "                      [96, 98, 100],\n",
    "                      [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomDataset at 0x7f0e340fe810>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset,\n",
    "                       batch_size = 2,\n",
    "                       shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수 다 만들고서 한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 hypothesis: tensor([185.3281, 195.9916]) Cost : 0.053858\n",
      "Epoch    0/100 hypothesis: tensor([142.7910, 150.4188]) Cost : 1.562913\n",
      "Epoch    0/100 hypothesis: 180.38885498046875 Cost : 0.151208\n",
      "Epoch    1/100 hypothesis: tensor([195.8915, 150.4081]) Cost : 1.272889\n",
      "Epoch    1/100 hypothesis: tensor([185.5946, 143.0548]) Cost : 0.733121\n",
      "Epoch    1/100 hypothesis: 180.2103729248047 Cost : 0.044257\n",
      "Epoch    2/100 hypothesis: tensor([195.7915, 142.7082]) Cost : 0.272525\n",
      "Epoch    2/100 hypothesis: tensor([185.0610, 180.0301]) Cost : 0.002316\n",
      "Epoch    2/100 hypothesis: 150.24664306640625 Cost : 3.074260\n",
      "Epoch    3/100 hypothesis: tensor([196.4660, 143.1865]) Cost : 0.812405\n",
      "Epoch    3/100 hypothesis: tensor([185.4161, 150.5629]) Cost : 1.119143\n",
      "Epoch    3/100 hypothesis: 180.5736083984375 Cost : 0.329027\n",
      "Epoch    4/100 hypothesis: tensor([185.3280, 142.8507]) Cost : 0.415632\n",
      "Epoch    4/100 hypothesis: tensor([180.0540, 150.2871]) Cost : 1.468428\n",
      "Epoch    4/100 hypothesis: 196.100830078125 Cost : 0.010167\n",
      "Epoch    5/100 hypothesis: tensor([142.8842, 150.5282]) Cost : 1.473898\n",
      "Epoch    5/100 hypothesis: tensor([185.5095, 196.1906]) Cost : 0.147936\n",
      "Epoch    5/100 hypothesis: 180.29893493652344 Cost : 0.089362\n",
      "Epoch    6/100 hypothesis: tensor([195.8406, 142.7398]) Cost : 0.286369\n",
      "Epoch    6/100 hypothesis: tensor([185.0847, 180.0567]) Cost : 0.005196\n",
      "Epoch    6/100 hypothesis: 150.26113891601562 Cost : 3.023638\n",
      "Epoch    7/100 hypothesis: tensor([180.7359, 196.4746]) Cost : 0.383392\n",
      "Epoch    7/100 hypothesis: tensor([142.9537, 150.6076]) Cost : 1.424222\n",
      "Epoch    7/100 hypothesis: 185.57164001464844 Cost : 0.326772\n",
      "Epoch    8/100 hypothesis: tensor([185.2853, 150.4601]) Cost : 1.226334\n",
      "Epoch    8/100 hypothesis: tensor([180.5019, 143.0067]) Cost : 0.632638\n",
      "Epoch    8/100 hypothesis: 195.8805389404297 Cost : 0.014271\n",
      "Epoch    9/100 hypothesis: tensor([185.2823, 195.9494]) Cost : 0.041137\n",
      "Epoch    9/100 hypothesis: tensor([142.7708, 150.4113]) Cost : 1.559119\n",
      "Epoch    9/100 hypothesis: 180.37879943847656 Cost : 0.143489\n",
      "Epoch   10/100 hypothesis: tensor([142.7679, 180.1947]) Cost : 0.313786\n",
      "Epoch   10/100 hypothesis: tensor([185.0269, 195.6761]) Cost : 0.052803\n",
      "Epoch   10/100 hypothesis: 150.3142547607422 Cost : 2.841737\n",
      "Epoch   11/100 hypothesis: tensor([143.2152, 185.8086]) Cost : 1.065242\n",
      "Epoch   11/100 hypothesis: tensor([180.3463, 150.5384]) Cost : 1.128062\n",
      "Epoch   11/100 hypothesis: 196.28480529785156 Cost : 0.081114\n",
      "Epoch   12/100 hypothesis: tensor([180.4114, 150.5937]) Cost : 1.073483\n",
      "Epoch   12/100 hypothesis: tensor([143.0791, 196.3260]) Cost : 0.635429\n",
      "Epoch   12/100 hypothesis: 185.3370361328125 Cost : 0.113593\n",
      "Epoch   13/100 hypothesis: tensor([195.8301, 180.1447]) Cost : 0.024908\n",
      "Epoch   13/100 hypothesis: tensor([142.7338, 150.3773]) Cost : 1.585752\n",
      "Epoch   13/100 hypothesis: 185.3755645751953 Cost : 0.141049\n",
      "Epoch   14/100 hypothesis: tensor([142.7406, 185.1875]) Cost : 0.291792\n",
      "Epoch   14/100 hypothesis: tensor([195.6495, 179.9790]) Cost : 0.061650\n",
      "Epoch   14/100 hypothesis: 150.31246948242188 Cost : 2.847759\n",
      "Epoch   15/100 hypothesis: tensor([185.8005, 150.8982]) Cost : 0.927361\n",
      "Epoch   15/100 hypothesis: tensor([196.5416, 180.7985]) Cost : 0.465471\n",
      "Epoch   15/100 hypothesis: 142.96847534179688 Cost : 0.937944\n",
      "Epoch   16/100 hypothesis: tensor([150.3307, 185.1168]) Cost : 1.400048\n",
      "Epoch   16/100 hypothesis: tensor([180.4107, 142.9284]) Cost : 0.515300\n",
      "Epoch   16/100 hypothesis: 195.8205108642578 Cost : 0.032216\n",
      "Epoch   17/100 hypothesis: tensor([150.4448, 185.2526]) Cost : 1.241235\n",
      "Epoch   17/100 hypothesis: tensor([142.9876, 180.4883]) Cost : 0.606918\n",
      "Epoch   17/100 hypothesis: 195.8722686767578 Cost : 0.016315\n",
      "Epoch   18/100 hypothesis: tensor([142.8041, 185.2724]) Cost : 0.360350\n",
      "Epoch   18/100 hypothesis: tensor([150.2782, 195.7081]) Cost : 1.524961\n",
      "Epoch   18/100 hypothesis: 180.46388244628906 Cost : 0.215187\n",
      "Epoch   19/100 hypothesis: tensor([180.2384, 185.2584]) Cost : 0.061794\n",
      "Epoch   19/100 hypothesis: tensor([150.3491, 195.7991]) Cost : 1.382912\n",
      "Epoch   19/100 hypothesis: 143.0019073486328 Cost : 1.003818\n",
      "Epoch   20/100 hypothesis: tensor([142.7097, 195.8156]) Cost : 0.268808\n",
      "Epoch   20/100 hypothesis: tensor([150.2914, 180.0474]) Cost : 1.460836\n",
      "Epoch   20/100 hypothesis: 185.40716552734375 Cost : 0.165784\n",
      "Epoch   21/100 hypothesis: tensor([180.1858, 142.7504]) Cost : 0.298804\n",
      "Epoch   21/100 hypothesis: tensor([150.2516, 185.0140]) Cost : 1.528573\n",
      "Epoch   21/100 hypothesis: 196.0582275390625 Cost : 0.003390\n",
      "Epoch   22/100 hypothesis: tensor([142.8567, 180.3242]) Cost : 0.419547\n",
      "Epoch   22/100 hypothesis: tensor([185.0993, 195.7634]) Cost : 0.032922\n",
      "Epoch   22/100 hypothesis: 150.35586547851562 Cost : 2.703178\n",
      "Epoch   23/100 hypothesis: tensor([196.5398, 185.8221]) Cost : 0.483661\n",
      "Epoch   23/100 hypothesis: tensor([180.4519, 150.6348]) Cost : 1.033916\n",
      "Epoch   23/100 hypothesis: 143.085693359375 Cost : 1.178730\n",
      "Epoch   24/100 hypothesis: tensor([195.9044, 180.2140]) Cost : 0.027461\n",
      "Epoch   24/100 hypothesis: tensor([185.2019, 142.7483]) Cost : 0.300387\n",
      "Epoch   24/100 hypothesis: 150.2513885498047 Cost : 3.057642\n",
      "Epoch   25/100 hypothesis: tensor([150.8583, 196.4494]) Cost : 0.752698\n",
      "Epoch   25/100 hypothesis: tensor([143.2443, 185.8528]) Cost : 1.137739\n",
      "Epoch   25/100 hypothesis: 180.38589477539062 Cost : 0.148915\n",
      "Epoch   26/100 hypothesis: tensor([142.7547, 180.1983]) Cost : 0.304462\n",
      "Epoch   26/100 hypothesis: tensor([185.0181, 195.6800]) Cost : 0.051351\n",
      "Epoch   26/100 hypothesis: 150.33016967773438 Cost : 2.788333\n",
      "Epoch   27/100 hypothesis: tensor([143.1991, 150.9098]) Cost : 1.313212\n",
      "Epoch   27/100 hypothesis: tensor([180.7723, 196.5119]) Cost : 0.429244\n",
      "Epoch   27/100 hypothesis: 185.46409606933594 Cost : 0.215385\n",
      "Epoch   28/100 hypothesis: tensor([195.9107, 142.7697]) Cost : 0.300191\n",
      "Epoch   28/100 hypothesis: tensor([180.0992, 185.1086]) Cost : 0.010813\n",
      "Epoch   28/100 hypothesis: 150.29766845703125 Cost : 2.897933\n",
      "Epoch   29/100 hypothesis: tensor([196.4843, 150.8885]) Cost : 0.734956\n",
      "Epoch   29/100 hypothesis: tensor([180.8472, 185.8664]) Cost : 0.734218\n",
      "Epoch   29/100 hypothesis: 142.92852783203125 Cost : 0.862164\n",
      "Epoch   30/100 hypothesis: tensor([180.0785, 185.0857]) Cost : 0.006756\n",
      "Epoch   30/100 hypothesis: tensor([195.7122, 150.2905]) Cost : 1.502522\n",
      "Epoch   30/100 hypothesis: 142.95655822753906 Cost : 0.915004\n",
      "Epoch   31/100 hypothesis: tensor([180.1056, 142.6775]) Cost : 0.235083\n",
      "Epoch   31/100 hypothesis: tensor([150.2190, 195.6188]) Cost : 1.658646\n",
      "Epoch   31/100 hypothesis: 185.42965698242188 Cost : 0.184605\n",
      "Epoch   32/100 hypothesis: tensor([195.8959, 150.4345]) Cost : 1.230843\n",
      "Epoch   32/100 hypothesis: tensor([143.0248, 180.5560]) Cost : 0.679665\n",
      "Epoch   32/100 hypothesis: 185.2351837158203 Cost : 0.055311\n",
      "Epoch   33/100 hypothesis: tensor([195.7925, 185.1174]) Cost : 0.028421\n",
      "Epoch   33/100 hypothesis: tensor([180.1382, 150.3768]) Cost : 1.326866\n",
      "Epoch   33/100 hypothesis: 142.93234252929688 Cost : 0.869263\n",
      "Epoch   34/100 hypothesis: tensor([180.0872, 150.3344]) Cost : 1.390842\n",
      "Epoch   34/100 hypothesis: tensor([142.9081, 185.4160]) Cost : 0.498894\n",
      "Epoch   34/100 hypothesis: 195.81661987304688 Cost : 0.033628\n",
      "Epoch   35/100 hypothesis: tensor([180.2316, 150.4570]) Cost : 1.217293\n",
      "Epoch   35/100 hypothesis: tensor([196.2059, 185.5007]) Cost : 0.146561\n",
      "Epoch   35/100 hypothesis: 142.83460998535156 Cost : 0.696574\n",
      "Epoch   36/100 hypothesis: tensor([142.5911, 185.0011]) Cost : 0.174724\n",
      "Epoch   36/100 hypothesis: tensor([179.8888, 195.5489]) Cost : 0.107918\n",
      "Epoch   36/100 hypothesis: 150.29196166992188 Cost : 2.917395\n",
      "Epoch   37/100 hypothesis: tensor([180.7365, 185.7471]) Cost : 0.550293\n",
      "Epoch   37/100 hypothesis: tensor([142.8785, 196.0763]) Cost : 0.388801\n",
      "Epoch   37/100 hypothesis: 150.4217987060547 Cost : 2.490719\n",
      "Epoch   38/100 hypothesis: tensor([180.8358, 143.2354]) Cost : 1.112353\n",
      "Epoch   38/100 hypothesis: tensor([196.1057, 150.6021]) Cost : 0.982655\n",
      "Epoch   38/100 hypothesis: 185.66708374023438 Cost : 0.445001\n",
      "Epoch   39/100 hypothesis: tensor([196.0294, 150.5435]) Cost : 1.061124\n",
      "Epoch   39/100 hypothesis: tensor([185.6281, 180.6216]) Cost : 0.390483\n",
      "Epoch   39/100 hypothesis: 142.83119201660156 Cost : 0.690880\n",
      "Epoch   40/100 hypothesis: tensor([150.2678, 184.9999]) Cost : 1.500326\n",
      "Epoch   40/100 hypothesis: tensor([180.3587, 196.0600]) Cost : 0.066140\n",
      "Epoch   40/100 hypothesis: 142.7837371826172 Cost : 0.614244\n",
      "Epoch   41/100 hypothesis: tensor([150.2328, 195.6268]) Cost : 1.631173\n",
      "Epoch   41/100 hypothesis: tensor([185.4241, 142.9118]) Cost : 0.505632\n",
      "Epoch   41/100 hypothesis: 180.14642333984375 Cost : 0.021440\n",
      "Epoch   42/100 hypothesis: tensor([180.0753, 185.0715]) Cost : 0.005389\n",
      "Epoch   42/100 hypothesis: tensor([150.2996, 142.6149]) Cost : 1.634762\n",
      "Epoch   42/100 hypothesis: 195.9662628173828 Cost : 0.001138\n",
      "Epoch   43/100 hypothesis: tensor([195.9857, 150.5123]) Cost : 1.106798\n",
      "Epoch   43/100 hypothesis: tensor([180.5998, 143.0476]) Cost : 0.728628\n",
      "Epoch   43/100 hypothesis: 185.25473022460938 Cost : 0.064887\n",
      "Epoch   44/100 hypothesis: tensor([142.6846, 150.3784]) Cost : 1.549223\n",
      "Epoch   44/100 hypothesis: tensor([185.3337, 196.0345]) Cost : 0.056271\n",
      "Epoch   44/100 hypothesis: 180.2442169189453 Cost : 0.059642\n",
      "Epoch   45/100 hypothesis: tensor([185.1204, 195.8057]) Cost : 0.026133\n",
      "Epoch   45/100 hypothesis: tensor([150.3922, 180.1472]) Cost : 1.303377\n",
      "Epoch   45/100 hypothesis: 142.92340087890625 Cost : 0.852669\n",
      "Epoch   46/100 hypothesis: tensor([180.0942, 142.6540]) Cost : 0.218322\n",
      "Epoch   46/100 hypothesis: tensor([150.2250, 195.6127]) Cost : 1.650248\n",
      "Epoch   46/100 hypothesis: 185.41285705566406 Cost : 0.170451\n",
      "Epoch   47/100 hypothesis: tensor([150.4479, 142.7440]) Cost : 1.481305\n",
      "Epoch   47/100 hypothesis: tensor([180.3903, 196.0938]) Cost : 0.080580\n",
      "Epoch   47/100 hypothesis: 185.26539611816406 Cost : 0.070435\n",
      "Epoch   48/100 hypothesis: tensor([180.1398, 195.8210]) Cost : 0.025795\n",
      "Epoch   48/100 hypothesis: tensor([185.1461, 142.6980]) Cost : 0.254242\n",
      "Epoch   48/100 hypothesis: 150.2579345703125 Cost : 3.034792\n",
      "Epoch   49/100 hypothesis: tensor([143.1208, 185.7010]) Cost : 0.873832\n",
      "Epoch   49/100 hypothesis: tensor([180.3176, 150.5390]) Cost : 1.117678\n",
      "Epoch   49/100 hypothesis: 196.2568359375 Cost : 0.065965\n",
      "Epoch   50/100 hypothesis: tensor([185.3990, 150.6125]) Cost : 1.042124\n",
      "Epoch   50/100 hypothesis: tensor([196.3117, 180.5908]) Cost : 0.223088\n",
      "Epoch   50/100 hypothesis: 142.85910034179688 Cost : 0.738053\n",
      "Epoch   51/100 hypothesis: tensor([185.0301, 180.0416]) Cost : 0.001318\n",
      "Epoch   51/100 hypothesis: tensor([195.6946, 150.2921]) Cost : 1.505130\n",
      "Epoch   51/100 hypothesis: 142.92831420898438 Cost : 0.861767\n",
      "Epoch   52/100 hypothesis: tensor([180.1064, 195.7841]) Cost : 0.028956\n",
      "Epoch   52/100 hypothesis: tensor([142.6817, 185.1266]) Cost : 0.240373\n",
      "Epoch   52/100 hypothesis: 150.25424194335938 Cost : 3.047671\n",
      "Epoch   53/100 hypothesis: tensor([196.4249, 150.8602]) Cost : 0.739855\n",
      "Epoch   53/100 hypothesis: tensor([180.8165, 185.8141]) Cost : 0.664740\n",
      "Epoch   53/100 hypothesis: 142.89649963378906 Cost : 0.803712\n",
      "Epoch   54/100 hypothesis: tensor([195.7554, 180.0802]) Cost : 0.033140\n",
      "Epoch   54/100 hypothesis: tensor([185.1122, 142.6700]) Cost : 0.230733\n",
      "Epoch   54/100 hypothesis: 150.2502899169922 Cost : 3.061485\n",
      "Epoch   55/100 hypothesis: tensor([196.4194, 150.8576]) Cost : 0.740483\n",
      "Epoch   55/100 hypothesis: tensor([185.8092, 180.8137]) Cost : 0.658414\n",
      "Epoch   55/100 hypothesis: 142.8935089111328 Cost : 0.798358\n",
      "Epoch   56/100 hypothesis: tensor([185.0645, 180.0804]) Cost : 0.005313\n",
      "Epoch   56/100 hypothesis: tensor([150.3134, 195.7167]) Cost : 1.462377\n",
      "Epoch   56/100 hypothesis: 142.930908203125 Cost : 0.866590\n",
      "Epoch   57/100 hypothesis: tensor([185.0997, 180.1161]) Cost : 0.011705\n",
      "Epoch   57/100 hypothesis: tensor([142.6185, 195.7366]) Cost : 0.225953\n",
      "Epoch   57/100 hypothesis: 150.29037475585938 Cost : 2.922818\n",
      "Epoch   58/100 hypothesis: tensor([196.4500, 180.7186]) Cost : 0.359436\n",
      "Epoch   58/100 hypothesis: tensor([142.8972, 150.6357]) Cost : 1.333154\n",
      "Epoch   58/100 hypothesis: 185.52442932128906 Cost : 0.275026\n",
      "Epoch   59/100 hypothesis: tensor([180.2776, 142.7823]) Cost : 0.344531\n",
      "Epoch   59/100 hypothesis: tensor([185.0438, 195.7360]) Cost : 0.035798\n",
      "Epoch   59/100 hypothesis: 150.38076782226562 Cost : 2.621913\n",
      "Epoch   60/100 hypothesis: tensor([143.1753, 150.9428]) Cost : 1.249500\n",
      "Epoch   60/100 hypothesis: tensor([196.5197, 185.7729]) Cost : 0.433718\n",
      "Epoch   60/100 hypothesis: 180.45480346679688 Cost : 0.206846\n",
      "Epoch   61/100 hypothesis: tensor([150.4762, 185.2155]) Cost : 1.184179\n",
      "Epoch   61/100 hypothesis: tensor([142.9469, 180.4934]) Cost : 0.570035\n",
      "Epoch   61/100 hypothesis: 195.87989807128906 Cost : 0.014424\n",
      "Epoch   62/100 hypothesis: tensor([142.7652, 195.9491]) Cost : 0.294047\n",
      "Epoch   62/100 hypothesis: tensor([150.3881, 185.1079]) Cost : 1.304997\n",
      "Epoch   62/100 hypothesis: 180.432861328125 Cost : 0.187369\n",
      "Epoch   63/100 hypothesis: tensor([142.7363, 185.2028]) Cost : 0.291598\n",
      "Epoch   63/100 hypothesis: tensor([150.3087, 180.0340]) Cost : 1.430852\n",
      "Epoch   63/100 hypothesis: 196.0730438232422 Cost : 0.005335\n",
      "Epoch   64/100 hypothesis: tensor([180.3343, 142.8220]) Cost : 0.393747\n",
      "Epoch   64/100 hypothesis: tensor([185.0761, 195.7741]) Cost : 0.028405\n",
      "Epoch   64/100 hypothesis: 150.39822387695312 Cost : 2.565687\n",
      "Epoch   65/100 hypothesis: tensor([143.1791, 196.5348]) Cost : 0.838086\n",
      "Epoch   65/100 hypothesis: tensor([185.4153, 180.4339]) Cost : 0.180358\n",
      "Epoch   65/100 hypothesis: 150.47222900390625 Cost : 2.334084\n",
      "Epoch   66/100 hypothesis: tensor([180.8533, 196.5958]) Cost : 0.541517\n",
      "Epoch   66/100 hypothesis: tensor([150.6942, 142.9388]) Cost : 1.293243\n",
      "Epoch   66/100 hypothesis: 185.5619354248047 Cost : 0.315771\n",
      "Epoch   67/100 hypothesis: tensor([185.2805, 150.5381]) Cost : 1.107924\n",
      "Epoch   67/100 hypothesis: tensor([180.5336, 142.9725]) Cost : 0.615225\n",
      "Epoch   67/100 hypothesis: 195.9071044921875 Cost : 0.008630\n",
      "Epoch   68/100 hypothesis: tensor([150.5112, 185.2469]) Cost : 1.138739\n",
      "Epoch   68/100 hypothesis: tensor([180.5150, 142.9573]) Cost : 0.590807\n",
      "Epoch   68/100 hypothesis: 195.894775390625 Cost : 0.011072\n",
      "Epoch   69/100 hypothesis: tensor([180.2655, 195.9554]) Cost : 0.036239\n",
      "Epoch   69/100 hypothesis: tensor([185.1879, 150.4632]) Cost : 1.198592\n",
      "Epoch   69/100 hypothesis: 142.9309539794922 Cost : 0.866675\n",
      "Epoch   70/100 hypothesis: tensor([180.1318, 142.6594]) Cost : 0.226076\n",
      "Epoch   70/100 hypothesis: tensor([195.6398, 184.9463]) Cost : 0.066295\n",
      "Epoch   70/100 hypothesis: 150.3555145263672 Cost : 2.704333\n",
      "Epoch   71/100 hypothesis: tensor([143.1440, 180.7595]) Cost : 0.942830\n",
      "Epoch   71/100 hypothesis: tensor([196.0579, 185.3349]) Cost : 0.057753\n",
      "Epoch   71/100 hypothesis: 150.50634765625 Cost : 2.230997\n",
      "Epoch   72/100 hypothesis: tensor([185.8570, 151.0248]) Cost : 0.842732\n",
      "Epoch   72/100 hypothesis: tensor([196.6067, 180.8640]) Cost : 0.557314\n",
      "Epoch   72/100 hypothesis: 142.93650817871094 Cost : 0.877048\n",
      "Epoch   73/100 hypothesis: tensor([150.4057, 195.8197]) Cost : 1.287106\n",
      "Epoch   73/100 hypothesis: tensor([180.5163, 185.4915]) Cost : 0.254067\n",
      "Epoch   73/100 hypothesis: 142.76206970214844 Cost : 0.580750\n",
      "Epoch   74/100 hypothesis: tensor([150.2727, 195.6472]) Cost : 1.553949\n",
      "Epoch   74/100 hypothesis: tensor([142.8860, 185.4040]) Cost : 0.474063\n",
      "Epoch   74/100 hypothesis: 180.16473388671875 Cost : 0.027137\n",
      "Epoch   75/100 hypothesis: tensor([150.3592, 142.6176]) Cost : 1.536871\n",
      "Epoch   75/100 hypothesis: tensor([195.9981, 180.3054]) Cost : 0.046624\n",
      "Epoch   75/100 hypothesis: 185.20089721679688 Cost : 0.040360\n",
      "Epoch   76/100 hypothesis: tensor([195.8100, 142.6539]) Cost : 0.231872\n",
      "Epoch   76/100 hypothesis: tensor([150.3389, 180.0599]) Cost : 1.381475\n",
      "Epoch   76/100 hypothesis: 185.357177734375 Cost : 0.127576\n",
      "Epoch   77/100 hypothesis: tensor([185.1783, 180.2102]) Cost : 0.037989\n",
      "Epoch   77/100 hypothesis: tensor([195.7909, 150.3860]) Cost : 1.324340\n",
      "Epoch   77/100 hypothesis: 142.93836975097656 Cost : 0.880538\n",
      "Epoch   78/100 hypothesis: tensor([195.8275, 180.1489]) Cost : 0.025960\n",
      "Epoch   78/100 hypothesis: tensor([142.6719, 185.1247]) Cost : 0.233529\n",
      "Epoch   78/100 hypothesis: 150.2906494140625 Cost : 2.921880\n",
      "Epoch   79/100 hypothesis: tensor([143.0923, 185.6763]) Cost : 0.825249\n",
      "Epoch   79/100 hypothesis: tensor([180.3309, 196.0255]) Cost : 0.055079\n",
      "Epoch   79/100 hypothesis: 150.4963836669922 Cost : 2.260862\n",
      "Epoch   80/100 hypothesis: tensor([185.8355, 143.2132]) Cost : 1.084972\n",
      "Epoch   80/100 hypothesis: tensor([196.1299, 180.4269]) Cost : 0.099571\n",
      "Epoch   80/100 hypothesis: 150.53536987304688 Cost : 2.145141\n",
      "Epoch   81/100 hypothesis: tensor([151.0437, 180.8901]) Cost : 0.853354\n",
      "Epoch   81/100 hypothesis: tensor([196.6123, 185.8432]) Cost : 0.542934\n",
      "Epoch   81/100 hypothesis: 142.93209838867188 Cost : 0.868807\n",
      "Epoch   82/100 hypothesis: tensor([180.1496, 150.4191]) Cost : 1.260890\n",
      "Epoch   82/100 hypothesis: tensor([185.4034, 142.8828]) Cost : 0.471042\n",
      "Epoch   82/100 hypothesis: 195.852294921875 Cost : 0.021817\n",
      "Epoch   83/100 hypothesis: tensor([195.9374, 150.5048]) Cost : 1.119781\n",
      "Epoch   83/100 hypothesis: tensor([142.9874, 185.5409]) Cost : 0.633752\n",
      "Epoch   83/100 hypothesis: 180.25494384765625 Cost : 0.064996\n",
      "Epoch   84/100 hypothesis: tensor([180.1310, 185.0911]) Cost : 0.012732\n",
      "Epoch   84/100 hypothesis: tensor([150.3586, 142.6021]) Cost : 1.528337\n",
      "Epoch   84/100 hypothesis: 195.99168395996094 Cost : 0.000069\n",
      "Epoch   85/100 hypothesis: tensor([150.5517, 180.3049]) Cost : 1.095260\n",
      "Epoch   85/100 hypothesis: tensor([185.4930, 142.9504]) Cost : 0.573133\n",
      "Epoch   85/100 hypothesis: 195.9124755859375 Cost : 0.007661\n",
      "Epoch   86/100 hypothesis: tensor([195.9630, 142.7534]) Cost : 0.284494\n",
      "Epoch   86/100 hypothesis: tensor([185.1010, 150.4151]) Cost : 1.261074\n",
      "Epoch   86/100 hypothesis: 180.44273376464844 Cost : 0.196013\n",
      "Epoch   87/100 hypothesis: tensor([195.9121, 142.7167]) Cost : 0.260671\n",
      "Epoch   87/100 hypothesis: tensor([185.0737, 150.3933]) Cost : 1.293396\n",
      "Epoch   87/100 hypothesis: 180.42764282226562 Cost : 0.182878\n",
      "Epoch   88/100 hypothesis: tensor([185.1787, 150.4813]) Cost : 1.169222\n",
      "Epoch   88/100 hypothesis: tensor([180.4875, 142.9167]) Cost : 0.539043\n",
      "Epoch   88/100 hypothesis: 195.87832641601562 Cost : 0.014804\n",
      "Epoch   89/100 hypothesis: tensor([185.2199, 195.9485]) Cost : 0.025497\n",
      "Epoch   89/100 hypothesis: tensor([142.7097, 180.2205]) Cost : 0.276175\n",
      "Epoch   89/100 hypothesis: 150.3245391845703 Cost : 2.807169\n",
      "Epoch   90/100 hypothesis: tensor([143.0962, 180.7213]) Cost : 0.860998\n",
      "Epoch   90/100 hypothesis: tensor([150.5842, 196.0341]) Cost : 1.002896\n",
      "Epoch   90/100 hypothesis: 185.58392333984375 Cost : 0.340966\n",
      "Epoch   91/100 hypothesis: tensor([150.5793, 142.7951]) Cost : 1.325322\n",
      "Epoch   91/100 hypothesis: tensor([196.1818, 185.4351]) Cost : 0.111181\n",
      "Epoch   91/100 hypothesis: 180.3202667236328 Cost : 0.102571\n",
      "Epoch   92/100 hypothesis: tensor([150.4371, 142.6636]) Cost : 1.441543\n",
      "Epoch   92/100 hypothesis: tensor([185.3177, 180.3606]) Cost : 0.115509\n",
      "Epoch   92/100 hypothesis: 195.87574768066406 Cost : 0.015439\n",
      "Epoch   93/100 hypothesis: tensor([195.9474, 180.2604]) Cost : 0.035295\n",
      "Epoch   93/100 hypothesis: tensor([185.1659, 150.4768]) Cost : 1.173848\n",
      "Epoch   93/100 hypothesis: 142.90875244140625 Cost : 0.825831\n",
      "Epoch   94/100 hypothesis: tensor([142.6437, 150.4184]) Cost : 1.457934\n",
      "Epoch   94/100 hypothesis: tensor([185.3000, 180.3447]) Cost : 0.104423\n",
      "Epoch   94/100 hypothesis: 195.86721801757812 Cost : 0.017631\n",
      "Epoch   95/100 hypothesis: tensor([150.5169, 180.2572]) Cost : 1.132826\n",
      "Epoch   95/100 hypothesis: tensor([196.2071, 185.4564]) Cost : 0.125582\n",
      "Epoch   95/100 hypothesis: 142.79006958007812 Cost : 0.624210\n",
      "Epoch   96/100 hypothesis: tensor([142.5596, 195.7013]) Cost : 0.201180\n",
      "Epoch   96/100 hypothesis: tensor([184.9578, 180.0085]) Cost : 0.000928\n",
      "Epoch   96/100 hypothesis: 150.31471252441406 Cost : 2.840194\n",
      "Epoch   97/100 hypothesis: tensor([185.6675, 143.0802]) Cost : 0.806169\n",
      "Epoch   97/100 hypothesis: tensor([150.5895, 180.3411]) Cost : 1.052954\n",
      "Epoch   97/100 hypothesis: 196.25973510742188 Cost : 0.067462\n",
      "Epoch   98/100 hypothesis: tensor([185.3638, 150.6482]) Cost : 0.979898\n",
      "Epoch   98/100 hypothesis: tensor([142.9931, 180.5981]) Cost : 0.672015\n",
      "Epoch   98/100 hypothesis: 195.95278930664062 Cost : 0.002229\n",
      "Epoch   99/100 hypothesis: tensor([142.7552, 150.5479]) Cost : 1.339432\n",
      "Epoch   99/100 hypothesis: tensor([196.1499, 185.3999]) Cost : 0.091199\n",
      "Epoch   99/100 hypothesis: 180.3087615966797 Cost : 0.095334\n",
      "Epoch  100/100 hypothesis: tensor([180.1587, 142.6521]) Cost : 0.225211\n",
      "Epoch  100/100 hypothesis: tensor([150.3007, 184.9435]) Cost : 1.445450\n",
      "Epoch  100/100 hypothesis: 196.05532836914062 Cost : 0.003061\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 100\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        pred = model(x_train)\n",
    "        \n",
    "        cost = F.mse_loss(pred, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        print(\"Epoch {:4d}/{} hypothesis: {} Cost : {:.6f}\".\n",
    "              format(epoch, nb_epochs, pred.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
